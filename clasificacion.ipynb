{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afb6238b",
   "metadata": {},
   "source": [
    "# Notebook de pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac439240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparando audio...\n",
      "Cargando modelo...\n",
      "Realizando predicción...\n",
      "\n",
      "🎵 Instrumentos detectados (solo piano, guitarra y bajo):\n",
      "  - Bass guitar: 0.779\n",
      "\n",
      "Instrumentos detectados:\n",
      "  - Music: 0.896\n",
      "  - Bass guitar: 0.779\n",
      "  - Musical instrument: 0.658\n",
      "  - Guitar: 0.645\n",
      "  - Plucked string instrument: 0.507\n",
      "\n",
      "Embedding shape: torch.Size([2048])\n",
      "Embedding guardado como embedding.npy\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import wget\n",
    "import os\n",
    "import librosa\n",
    "import csv\n",
    "import sys\n",
    "sys.path.append('./audioset_tagging_cnn/pytorch')\n",
    "\n",
    "# ======= CONFIGURACIÓN =======\n",
    "MODEL_URL = 'https://zenodo.org/record/3987831/files/Cnn14_16k_mAP%3D0.438.pth?download=1'\n",
    "CHECKPOINT_PATH = 'Cnn14_16k.pth'  # Usa el nombre original\n",
    "LABELS_CSV = 'class_labels_indices.csv'\n",
    "AUDIO_PATH = 'About-the-Blues-Feel-the-Groove-_feat.-Priscilla-Zamborlini_-bass-E-minor-95bpm-440hz.wav'  # Cambia esto por tu archivo .wav\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "# ======= DESCARGAR PESOS Y LABELS =======\n",
    "if not os.path.exists(CHECKPOINT_PATH):\n",
    "    raise FileNotFoundError(\n",
    "        f\"No se encontró el modelo {CHECKPOINT_PATH}. \"\n",
    "        \"Por favor descárgalo manualmente desde:\\n\"\n",
    "        \"https://zenodo.org/record/3987831\\n\"\n",
    "        \"y colócalo en la misma carpeta con el nombre 'Cnn14_16k.pth'\"\n",
    "    )\n",
    "\n",
    "\n",
    "if not os.path.exists(LABELS_CSV):\n",
    "    print('Descargando etiquetas...')\n",
    "    wget.download(\"https://raw.githubusercontent.com/qiuqiangkong/audioset_tagging_cnn/master/metadata/class_labels_indices.csv\", LABELS_CSV)\n",
    "\n",
    "# ======= FUNCIONES UTILES =======\n",
    "def load_labels(csv_path):\n",
    "    with open(csv_path) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        return [row['display_name'] for row in reader]\n",
    "\n",
    "def preprocess_audio(wav_path, target_sample_rate=16000):\n",
    "    waveform, sr = torchaudio.load(wav_path)\n",
    "    waveform = waveform.mean(dim=0)  # convertir a mono\n",
    "    waveform = torchaudio.functional.resample(waveform, sr, target_sample_rate)\n",
    "    return waveform.unsqueeze(0)  # [1, T]\n",
    "\n",
    "# ======= MODELO CNN14 =======\n",
    "class Cnn14(torch.nn.Module):\n",
    "    def __init__(self, sample_rate=16000, window_size=512, hop_size=160,\n",
    "                 mel_bins=64, fmin=50, fmax=8000, classes_num=527):\n",
    "        super().__init__()\n",
    "        import sys\n",
    "        sys.path.append('.')  # Asegura que podemos importar localmente\n",
    "        from audioset_tagging_cnn.pytorch.models import Cnn14  # Asume que copiaste el modelo desde el repo oficial\n",
    "        self.model = Cnn14(\n",
    "            sample_rate=sample_rate,\n",
    "            window_size=window_size,\n",
    "            hop_size=hop_size,\n",
    "            mel_bins=mel_bins,\n",
    "            fmin=fmin,\n",
    "            fmax=fmax,\n",
    "            classes_num=classes_num\n",
    "        )\n",
    "        self.model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=DEVICE)['model'])\n",
    "        self.model.to(DEVICE).eval()\n",
    "\n",
    "    def forward(self, waveform):\n",
    "        with torch.no_grad():\n",
    "            output = self.model(waveform.to(DEVICE))\n",
    "        return output['clipwise_output'][0], output['embedding'][0]\n",
    "    \n",
    "def softmax_on_instruments(scores, labels, instrument_keywords=[\"piano\", \"guitar\", \"bass\"]):\n",
    "    indices = []\n",
    "    instrument_labels = []\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        label_lower = label.lower()\n",
    "        if any(kw in label_lower for kw in instrument_keywords):\n",
    "            indices.append(i)\n",
    "            instrument_labels.append(label)\n",
    "\n",
    "    if not indices:\n",
    "        return []\n",
    "\n",
    "    selected_scores = scores[indices]\n",
    "    softmaxed = torch.softmax(selected_scores, dim=0)\n",
    "\n",
    "    return list(zip(instrument_labels, softmaxed.tolist()))\n",
    "\n",
    "def mostrar_resultados_filtrados(scores, csv_path):\n",
    "    instrumentos_validos = [\"piano\", \"guitar\", \"bass\"]\n",
    "    \n",
    "    # Cargar etiquetas\n",
    "    with open(csv_path, newline='') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        labels = [row[\"display_name\"] for row in reader]\n",
    "    \n",
    "    resultados = []\n",
    "    for i, score in enumerate(scores):\n",
    "        nombre = labels[i].lower()\n",
    "        if any(instr in nombre for instr in instrumentos_validos):\n",
    "            resultados.append((labels[i], score.item()))\n",
    "\n",
    "    # Ordenar por score descendente y mostrar top\n",
    "    resultados.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(\"\\n🎵 Instrumentos detectados (solo piano, guitarra y bajo):\")\n",
    "    if resultados:\n",
    "        for label, score in resultados[:1]:\n",
    "            print(f\"  - {label}: {score:.3f}\")\n",
    "    else:\n",
    "        print(\"  - Ninguno de los instrumentos especificados fue detectado.\")\n",
    "\n",
    "\n",
    "# ======= MAIN =======\n",
    "if __name__ == '__main__':\n",
    "    print('Preparando audio...')\n",
    "    audio = preprocess_audio(AUDIO_PATH)\n",
    "    print('Cargando modelo...')\n",
    "    model = Cnn14()\n",
    "\n",
    "    print('Realizando predicción...')\n",
    "    scores, embedding = model(audio)\n",
    "    mostrar_resultados_filtrados(scores, LABELS_CSV)\n",
    "    \n",
    "    labels = load_labels(LABELS_CSV)\n",
    "    top_k = torch.topk(scores, k=5)\n",
    "    print('\\nInstrumentos detectados:')\n",
    "    for i in range(5):\n",
    "        print(f'  - {labels[top_k.indices[i]]}: {top_k.values[i].item():.3f}')\n",
    "\n",
    "    print(f'\\nEmbedding shape: {embedding.shape}')\n",
    "    np.save(\"embedding.npy\", embedding.cpu().numpy())\n",
    "    print('Embedding guardado como embedding.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50a048e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎵 Instrumentos detectados (solo piano, guitarra y bajo):\n",
      "  - Guitar: 0.280\n"
     ]
    }
   ],
   "source": [
    "audio_guitar = preprocess_audio('guitar_audio.wav')\n",
    "\n",
    "scores, embedding = model(audio_guitar)\n",
    "mostrar_resultados_filtrados(scores, LABELS_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cc626be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎵 Instrumentos detectados (solo piano, guitarra y bajo):\n",
      "  - Bass drum: 0.225\n"
     ]
    }
   ],
   "source": [
    "drum_audio = preprocess_audio('drum_audio.wav')\n",
    "\n",
    "scores, embedding = model(drum_audio)\n",
    "mostrar_resultados_filtrados(scores, LABELS_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed49f0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎵 Instrumentos detectados (solo piano, guitarra y bajo):\n",
      "  - Piano: 0.069\n"
     ]
    }
   ],
   "source": [
    "piano_audio = preprocess_audio('piano_audio.wav')\n",
    "\n",
    "scores, embedding = model(piano_audio)\n",
    "mostrar_resultados_filtrados(scores, LABELS_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43db5147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎵 Instrumentos detectados (solo piano, guitarra y bajo):\n",
      "  - Electric piano: 0.149\n"
     ]
    }
   ],
   "source": [
    "piano_audio2 = preprocess_audio('piano_audio2.wav')\n",
    "\n",
    "scores, embedding = model(piano_audio2)\n",
    "mostrar_resultados_filtrados(scores, LABELS_CSV)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
